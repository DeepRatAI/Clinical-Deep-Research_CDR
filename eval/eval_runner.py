#!/usr/bin/env python3
"""
CDR Evaluation Runner

Runs the CDR evaluation pipeline against a dataset of clinical questions
and produces structured results with quality metrics.

Modes:
    --mode offline   Structural validation only (no LLM calls, fast)
    --mode online    Full pipeline execution (requires LLM provider + .env)

Usage:
    # Offline (structural checks, no API keys needed):
    python -m eval.eval_runner --dataset eval/datasets/golden_set_toy.json --output eval/results/

    # Online (full pipeline, requires .env):
    python -m eval.eval_runner --mode online --dataset eval/datasets/golden_set_toy.json --output eval/results/

    # Compare with baseline:
    python -m eval.eval_runner --dataset eval/datasets/golden_set_toy.json --compare-baseline eval/results/baseline_v0_1.json

    # Single question:
    python -m eval.eval_runner --question GS-001

    # Output only JSON (no markdown summary):
    python -m eval.eval_runner --format json

    # Output only Markdown summary (no JSON):
    python -m eval.eval_runner --format markdown

Requirements:
    - CDR installed (pip install -e ".[dev]")
    - For online mode: .env configured with at least one LLM provider
    - PYTHONPATH includes src/
"""

from __future__ import annotations

import argparse
import hashlib
import json
import statistics
import sys
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).resolve().parent.parent / "src"))

from cdr.evaluation.golden_set import get_golden_set, get_question_by_id, GoldenSetQuestion
from cdr.evaluation.metrics import CDRMetricsEvaluator, EvaluationReport

# ============================================================================
# Constants
# ============================================================================

CDR_DISCLAIMER = (
    "âš ï¸ This report is machine-generated by CDR (Clinical Deep Research). "
    "It is NOT medical advice and should NOT be used for clinical decision-making. "
    "All findings require independent verification by qualified professionals. "
    "See DISCLAIMER.md for full terms."
)

PIPELINE_STAGES = [
    "parse_question",
    "plan_search",
    "retrieve",
    "deduplicate",
    "screen",
    "parse_docs",
    "extract_data",
    "assess_rob2",
    "synthesize",
    "critique",
    "verify",
    "compose",
    "publish",
]


# ============================================================================
# Dataset utilities
# ============================================================================


def load_dataset(dataset_path: str) -> tuple[list[dict[str, Any]], str]:
    """Load evaluation dataset and compute its checksum.

    Args:
        dataset_path: Path to evaluation dataset JSON file.

    Returns:
        Tuple of (questions list, sha256 hex digest of the file).
    """
    path = Path(dataset_path)
    if not path.exists():
        print(f"âŒ  Dataset not found: {dataset_path}")
        sys.exit(1)

    raw_bytes = path.read_bytes()
    data = json.loads(raw_bytes)
    questions = data.get("questions", data) if isinstance(data, dict) else data

    # Hash the questions array only (not the full file) to avoid
    # chicken-and-egg: the file contains its own sha256_questions field.
    questions_canonical = json.dumps(questions, sort_keys=True, ensure_ascii=False).encode("utf-8")
    checksum = hashlib.sha256(questions_canonical).hexdigest()

    # Verify against declared checksum if present
    declared = data.get("sha256_questions") if isinstance(data, dict) else None
    if declared and declared != checksum:
        print(f"âš ï¸   Checksum mismatch!")
        print(f"     Declared: {declared}")
        print(f"     Computed: {checksum}")

    return questions, checksum


def load_baseline(baseline_path: str) -> dict[str, Any] | None:
    """Load a baseline results file for comparison.

    Args:
        baseline_path: Path to baseline JSON.

    Returns:
        Baseline dict or None if not found.
    """
    path = Path(baseline_path)
    if not path.exists():
        print(f"âš ï¸   Baseline not found: {baseline_path} â€” skipping comparison")
        return None
    with open(path) as f:
        return json.load(f)


# ============================================================================
# Offline evaluation (structural, no LLM)
# ============================================================================


def evaluate_question_offline(question: dict[str, Any]) -> dict[str, Any]:
    """Evaluate a single question using offline structural metrics.

    Checks:
        - Dataset question has required fields (id, question, PICO components)
        - Golden set match (expected evidence level, min studies)

    This mode does NOT execute the pipeline â€” no LLM calls, no network.
    """
    qid = question.get("id", "unknown")
    golden = get_question_by_id(qid)

    # Structural checks
    checks: dict[str, bool] = {
        "has_id": bool(question.get("id")),
        "has_question": bool(question.get("question")),
        "has_population": bool(question.get("population")),
        "has_intervention": bool(question.get("intervention")),
        "has_outcome": bool(question.get("outcome")),
        "has_expected_evidence_level": bool(question.get("expected_evidence_level")),
    }
    all_pass = all(checks.values())

    result: dict[str, Any] = {
        "question_id": qid,
        "question": question.get("question", ""),
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "mode": "offline",
        "structural_checks": checks,
        "structural_pass": all_pass,
        "metrics": {
            "snippet_coverage": None,
            "verification_coverage": None,
            "composition_emitted": None,
            "pipeline_status": None,
        },
        "latency_ms": {},
        "status": "pass" if all_pass else "fail",
    }

    if golden:
        result["expected"] = {
            "evidence_level": golden.expected_evidence_level.value,
            "min_studies": golden.expected_min_studies,
            "composition_expected": golden.composition_expected,
            "min_verification_coverage": golden.min_verification_coverage,
        }

    return result


# ============================================================================
# Online evaluation (full pipeline)
# ============================================================================


def evaluate_question_online(question: dict[str, Any]) -> dict[str, Any]:
    """Evaluate a single question by running the full CDR pipeline.

    Requires:
        - .env configured with at least one LLM provider
        - Network access to PubMed/CT.gov

    Captures:
        - Per-stage latency (ms)
        - Output metrics (studies, claims, snippets, coverage)
        - Pipeline status
        - Token estimates
    """
    import asyncio

    qid = question.get("id", "unknown")
    golden = get_question_by_id(qid)
    question_text = question.get("question", "")

    stage_latencies: dict[str, float] = {}
    t_start = time.perf_counter()

    try:
        from cdr.orchestration.graph import run_graph

        result_state = asyncio.get_event_loop().run_until_complete(run_graph(question_text))
    except RuntimeError:
        # No event loop â€” create one
        try:
            from cdr.orchestration.graph import run_graph

            result_state = asyncio.run(run_graph(question_text))
        except Exception as e:
            return {
                "question_id": qid,
                "question": question_text,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "mode": "online",
                "status": "error",
                "error": str(e),
                "metrics": {},
                "latency_ms": {},
            }
    except Exception as e:
        return {
            "question_id": qid,
            "question": question_text,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "mode": "online",
            "status": "error",
            "error": str(e),
            "metrics": {},
            "latency_ms": {},
        }

    t_end = time.perf_counter()
    total_ms = round((t_end - t_start) * 1000, 1)

    # Extract report from result
    report = result_state if isinstance(result_state, dict) else {}
    if hasattr(result_state, "report"):
        report = result_state.report or {}

    # Extract metrics from report
    run_kpis = report.get("run_kpis", {})
    prisma = report.get("prisma_counts", {})
    claims = report.get("claims", [])

    result: dict[str, Any] = {
        "question_id": qid,
        "question": question_text,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "mode": "online",
        "status": report.get("status", "unknown"),
        "metrics": {
            "studies_found": report.get("study_count", 0),
            "claims_generated": report.get("claim_count", 0),
            "snippet_count": report.get("snippet_count", 0),
            "snippet_coverage": run_kpis.get("snippet_coverage", None),
            "verification_coverage": run_kpis.get("verification_coverage", None),
            "composition_emitted": bool(report.get("composed_hypotheses")),
            "pipeline_status": report.get("status", "unknown"),
            "records_identified": prisma.get("records_identified", 0),
            "records_screened": prisma.get("records_screened", 0),
            "studies_included": prisma.get("studies_included", 0),
            "claims_with_snippets": sum(1 for c in claims if c.get("supporting_snippet_ids")),
            "claims_with_snippets_pct": (
                round(
                    sum(1 for c in claims if c.get("supporting_snippet_ids")) / max(len(claims), 1),
                    4,
                )
            ),
        },
        "latency_ms": {
            "total": total_ms,
        },
        "token_estimate": {
            "note": "Estimated; actual depends on provider response headers",
            "input_tokens_est": max(len(claims) * 3000, 15000),
            "output_tokens_est": max(len(claims) * 1500, 5000),
        },
    }

    if golden:
        result["expected"] = {
            "evidence_level": golden.expected_evidence_level.value,
            "min_studies": golden.expected_min_studies,
            "composition_expected": golden.composition_expected,
            "min_verification_coverage": golden.min_verification_coverage,
        }

    return result


# ============================================================================
# Baseline comparison
# ============================================================================


def compare_with_baseline(
    current_results: list[dict[str, Any]],
    baseline: dict[str, Any],
) -> list[dict[str, Any]]:
    """Compare current evaluation results against a baseline.

    For each question, compares metrics and flags regressions.

    Args:
        current_results: List of current evaluation result dicts.
        baseline: Baseline dict with 'results' key.

    Returns:
        List of comparison dicts per question.
    """
    baseline_by_id: dict[str, dict[str, Any]] = {}
    for br in baseline.get("results", []):
        baseline_by_id[br.get("question_id", "")] = br

    comparisons = []
    for result in current_results:
        qid = result.get("question_id", "")
        bl = baseline_by_id.get(qid)
        if not bl:
            comparisons.append(
                {
                    "question_id": qid,
                    "comparison": "no_baseline",
                    "message": f"No baseline data for {qid}",
                }
            )
            continue

        bl_metrics = bl.get("metrics", {})
        cur_metrics = result.get("metrics", {})

        diffs: dict[str, dict[str, Any]] = {}
        for key in [
            "snippet_coverage",
            "verification_coverage",
            "studies_found",
            "claims_generated",
        ]:
            bv = bl_metrics.get(key)
            cv = cur_metrics.get(key)
            if bv is not None and cv is not None:
                delta = (
                    cv - bv
                    if isinstance(cv, (int, float)) and isinstance(bv, (int, float))
                    else None
                )
                diffs[key] = {
                    "baseline": bv,
                    "current": cv,
                    "delta": round(delta, 4) if delta is not None else None,
                    "regression": delta is not None and delta < -0.05,
                }

        any_regression = any(d.get("regression", False) for d in diffs.values())
        comparisons.append(
            {
                "question_id": qid,
                "comparison": "regression" if any_regression else "ok",
                "diffs": diffs,
            }
        )

    return comparisons


# ============================================================================
# Aggregate statistics
# ============================================================================


def compute_aggregate(results: list[dict[str, Any]]) -> dict[str, Any]:
    """Compute aggregate statistics across all evaluation results.

    Includes p50/p95 latencies when available.
    """
    agg: dict[str, Any] = {
        "total_questions": len(results),
        "passed": sum(1 for r in results if r.get("status") in ("pass", "completed")),
        "failed": sum(1 for r in results if r.get("status") in ("fail", "error", "unpublishable")),
    }

    # Collect numeric metrics for averaging
    cov_vals = [
        r["metrics"]["verification_coverage"]
        for r in results
        if r.get("metrics", {}).get("verification_coverage") is not None
    ]
    if cov_vals:
        agg["avg_verification_coverage"] = round(statistics.mean(cov_vals), 4)

    snippet_vals = [
        r["metrics"]["snippet_coverage"]
        for r in results
        if r.get("metrics", {}).get("snippet_coverage") is not None
    ]
    if snippet_vals:
        agg["avg_snippet_coverage"] = round(statistics.mean(snippet_vals), 4)

    # Latency p50/p95
    latency_vals = [
        r["latency_ms"]["total"]
        for r in results
        if r.get("latency_ms", {}).get("total") is not None
    ]
    if latency_vals:
        sorted_lat = sorted(latency_vals)
        n = len(sorted_lat)
        agg["latency_p50_ms"] = round(sorted_lat[n // 2], 1)
        agg["latency_p95_ms"] = round(sorted_lat[min(int(n * 0.95), n - 1)], 1)

    # Token estimates
    token_vals = [
        r.get("token_estimate", {}).get("input_tokens_est", 0)
        + r.get("token_estimate", {}).get("output_tokens_est", 0)
        for r in results
        if r.get("token_estimate")
    ]
    if token_vals:
        agg["avg_tokens_per_request"] = round(statistics.mean(token_vals))

    # Claims with citations %
    cit_vals = [
        r["metrics"]["claims_with_snippets_pct"]
        for r in results
        if r.get("metrics", {}).get("claims_with_snippets_pct") is not None
    ]
    if cit_vals:
        agg["avg_claims_with_citations_pct"] = round(statistics.mean(cit_vals), 4)

    return agg


# ============================================================================
# Output generation
# ============================================================================


def generate_markdown_summary(
    summary: dict[str, Any],
    comparisons: list[dict[str, Any]] | None = None,
) -> str:
    """Generate a human-readable Markdown summary of evaluation results."""
    lines = [
        "# CDR Evaluation Summary",
        "",
        f"**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M')}  ",
        f"**Dataset**: {summary.get('dataset', 'N/A')}  ",
        f"**Dataset checksum**: `sha256:{summary.get('dataset_checksum', 'N/A')}`  ",
        f"**Mode**: {summary.get('mode', 'offline')}  ",
        f"**Questions evaluated**: {summary.get('total_questions', 0)}",
        "",
    ]

    # Aggregate
    agg = summary.get("aggregate", {})
    if agg:
        lines.extend(
            [
                "## Aggregate",
                "",
                "| Metric | Value |",
                "|--------|-------|",
            ]
        )
        for k, v in agg.items():
            lines.append(f"| {k} | {v} |")
        lines.append("")

    # Per-question
    lines.extend(
        [
            "## Per-Question Results",
            "",
            "| ID | Question | Status | Verif Cov | Latency (ms) |",
            "|----|----------|--------|-----------|-------------|",
        ]
    )
    for r in summary.get("results", []):
        m = r.get("metrics", {})
        lat = r.get("latency_ms", {}).get("total", "â€”")
        vc = m.get("verification_coverage", "â€”")
        if isinstance(vc, float):
            vc = f"{vc:.2%}"
        lines.append(
            f"| {r.get('question_id', '?')} "
            f"| {r.get('question', '')[:45]}... "
            f"| {r.get('status', '?')} "
            f"| {vc} "
            f"| {lat} |"
        )
    lines.append("")

    # Baseline comparison
    if comparisons:
        lines.extend(
            [
                "## Baseline Comparison",
                "",
                "| ID | Metric | Baseline | Current | Delta | Regression? |",
                "|----|--------|----------|---------|-------|-------------|",
            ]
        )
        for comp in comparisons:
            qid = comp.get("question_id", "?")
            if comp.get("comparison") == "no_baseline":
                lines.append(f"| {qid} | â€” | â€” | â€” | â€” | No baseline |")
                continue
            for metric, d in comp.get("diffs", {}).items():
                reg = "âš ï¸ YES" if d.get("regression") else "âœ… No"
                lines.append(
                    f"| {qid} | {metric} | {d.get('baseline', 'â€”')} "
                    f"| {d.get('current', 'â€”')} | {d.get('delta', 'â€”')} | {reg} |"
                )
        lines.append("")

    lines.extend(
        [
            "---",
            f"*Generated by CDR eval runner v{summary.get('eval_version', '0.1.0')}*",
        ]
    )

    return "\n".join(lines)


# ============================================================================
# Main runner
# ============================================================================


def run_evaluation(
    dataset_path: str,
    output_dir: str,
    question_id: str | None = None,
    mode: str = "offline",
    baseline_path: str | None = None,
    output_format: str = "all",
) -> dict[str, Any]:
    """Run evaluation on a dataset.

    Args:
        dataset_path: Path to evaluation dataset JSON.
        output_dir: Directory for results.
        question_id: Optional single question to evaluate.
        mode: 'offline' (structural only) or 'online' (full pipeline).
        baseline_path: Optional path to baseline JSON for comparison.
        output_format: 'json', 'markdown', or 'all' (default).

    Returns:
        Evaluation summary dict.
    """
    questions, checksum = load_dataset(dataset_path)

    if question_id:
        questions = [q for q in questions if q.get("id") == question_id]
        if not questions:
            print(f"âŒ  Question {question_id} not found in dataset")
            sys.exit(1)

    print(f"ðŸ“Š  CDR Evaluation Runner")
    print(f"    Mode:     {mode}")
    print(f"    Dataset:  {dataset_path}")
    print(f"    Checksum: sha256:{checksum[:16]}...")
    print(f"    Questions: {len(questions)}")
    print()

    # Evaluate each question
    results: list[dict[str, Any]] = []
    for i, q in enumerate(questions, 1):
        qid = q.get("id", f"Q-{i}")
        print(f"  [{i}/{len(questions)}] {qid}: {q.get('question', '')[:55]}...")

        if mode == "online":
            result = evaluate_question_online(q)
        else:
            result = evaluate_question_offline(q)

        results.append(result)
        status = result.get("status", "?")
        latency = result.get("latency_ms", {}).get("total", "â€”")
        print(f"           â†’ {status}  (latency: {latency}ms)")

    # Aggregate
    aggregate = compute_aggregate(results)

    # Baseline comparison
    comparisons: list[dict[str, Any]] | None = None
    if baseline_path:
        baseline = load_baseline(baseline_path)
        if baseline:
            comparisons = compare_with_baseline(results, baseline)
            regressions = sum(1 for c in comparisons if c.get("comparison") == "regression")
            print()
            if regressions:
                print(f"âš ï¸   {regressions} regression(s) detected vs baseline!")
            else:
                print(f"âœ…  No regressions vs baseline")

    # Build summary
    summary: dict[str, Any] = {
        "eval_version": "0.1.0",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "mode": mode,
        "dataset": str(dataset_path),
        "dataset_checksum": checksum,
        "total_questions": len(questions),
        "aggregate": aggregate,
        "results": results,
    }
    if comparisons is not None:
        summary["baseline_comparison"] = comparisons

    # Write output
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    ts = datetime.now().strftime("%Y%m%d_%H%M%S")

    print()
    print(f"âœ…  Evaluation complete")

    if output_format in ("json", "all"):
        results_file = output_path / f"{ts}_results.json"
        with open(results_file, "w") as f:
            json.dump(summary, f, indent=2, default=str)
        print(f"    Results: {results_file}")

    if output_format in ("markdown", "all"):
        summary_file = output_path / f"{ts}_summary.md"
        md_content = generate_markdown_summary(summary, comparisons)
        with open(summary_file, "w") as f:
            f.write(md_content)
        print(f"    Summary: {summary_file}")

    return summary


def main() -> None:
    parser = argparse.ArgumentParser(
        description="CDR Evaluation Runner",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Offline evaluation (fast, no API keys needed):
  python -m eval.eval_runner

  # Full pipeline evaluation (requires .env):
  python -m eval.eval_runner --mode online

  # Compare with baseline:
  python -m eval.eval_runner --compare-baseline eval/results/baseline_v0_1.json

  # Single question:
  python -m eval.eval_runner --question GS-001 --mode online

  # Output only JSON:
  python -m eval.eval_runner --format json

  # Output only Markdown summary:
  python -m eval.eval_runner --format markdown
        """,
    )
    parser.add_argument(
        "--dataset",
        default="eval/datasets/golden_set_toy.json",
        help="Path to evaluation dataset JSON (default: golden set)",
    )
    parser.add_argument(
        "--output",
        default="eval/results/",
        help="Output directory for results (default: eval/results/)",
    )
    parser.add_argument(
        "--question",
        default=None,
        help="Evaluate a single question by ID (e.g., GS-001)",
    )
    parser.add_argument(
        "--mode",
        choices=["offline", "online"],
        default="offline",
        help="Evaluation mode: 'offline' (structural) or 'online' (full pipeline)",
    )
    parser.add_argument(
        "--compare-baseline",
        default=None,
        dest="compare_baseline",
        help="Path to baseline JSON for regression comparison",
    )
    parser.add_argument(
        "--format",
        choices=["json", "markdown", "all"],
        default="all",
        dest="output_format",
        help="Output format: 'json', 'markdown', or 'all' (default: all)",
    )
    args = parser.parse_args()

    run_evaluation(
        args.dataset,
        args.output,
        args.question,
        args.mode,
        args.compare_baseline,
        args.output_format,
    )


if __name__ == "__main__":
    main()
