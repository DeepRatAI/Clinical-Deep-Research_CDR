#!/usr/bin/env python3
"""
CDR Online Canary ‚Äî ‚â•5 real clinical queries through the full pipeline.

Runs each query end-to-end with a real LLM provider, validates invariants
(schema, disclaimer, export, coherence), and produces a structured summary.

PASS/FAIL policy:
  FAIL (hard): crash, schema invalid, missing disclaimer, empty output,
               export not generated, claim_count != len(claims)
  WARN (soft): latency > 600s, low study count, coverage drift

Usage:
    PYTHONPATH=src python scripts/online_canary.py --output canary_output/
    PYTHONPATH=src python scripts/online_canary.py --provider openrouter --output canary_output/
"""

from __future__ import annotations

import argparse
import asyncio
import json
import os
import re
import sys
import time
from pathlib import Path

ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(ROOT / "src"))

# ‚îÄ‚îÄ Secret sanitization ‚îÄ‚îÄ
SECRET_PATTERNS = [
    re.compile(r"sk-or-v1-[A-Za-z0-9_-]{20,}"),
    re.compile(r"gsk_[A-Za-z0-9]{20,}"),
    re.compile(r"csk-[A-Za-z0-9]{20,}"),
    re.compile(r"AIzaSy[A-Za-z0-9_-]{20,}"),
    re.compile(r"hf_[A-Za-z0-9]{20,}"),
    re.compile(r"sk-[A-Za-z0-9_-]{20,}"),
    re.compile(r"ghp_[A-Za-z0-9]{20,}"),
    re.compile(r"github_pat_[A-Za-z0-9_]{20,}"),
]

# ‚îÄ‚îÄ Canary queries (‚â•5, diverse topics) ‚îÄ‚îÄ
CANARY_QUERIES = [
    {
        "id": "CQ-001",
        "question": "What is the efficacy of metformin versus sulfonylureas for glycemic control in type 2 diabetes?",
    },
    {
        "id": "CQ-002",
        "question": "What are the cardiovascular outcomes of PCSK9 inhibitors compared to statins in hypercholesterolemia?",
    },
    {
        "id": "CQ-003",
        "question": "What is the safety profile of JAK inhibitors versus biologics for rheumatoid arthritis?",
    },
    {
        "id": "CQ-004",
        "question": "What is the comparative effectiveness of immune checkpoint inhibitors for non-small cell lung cancer?",
    },
    {
        "id": "CQ-005",
        "question": "What are the neurological outcomes of direct oral anticoagulants versus warfarin in atrial fibrillation?",
    },
]

PROVIDER_ORDER = ["openrouter", "groq", "cerebras", "gemini"]

KEY_MAP = {
    "openrouter": "OPENROUTER_API_KEY",
    "groq": "GROQ_API_KEY",
    "cerebras": "CEREBRAS_API_KEY",
    "gemini": "GEMINI_API_KEY",
}

DISCLAIMER_CONST = (
    "‚ö†Ô∏è This report is machine-generated by CDR (Clinical Deep Research). "
    "It is NOT medical advice and should NOT be used for clinical decision-making. "
    "All findings require independent verification by qualified professionals. "
    "See DISCLAIMER.md for full terms."
)


def sanitize_text(text: str) -> str:
    """Remove any leaked secrets from text."""
    for pat in SECRET_PATTERNS:
        text = pat.sub("[REDACTED]", text)
    return text


def pick_provider(preferred: str = "auto"):
    """Select an available LLM provider."""
    from cdr.llm.factory import create_provider

    order = PROVIDER_ORDER
    if preferred != "auto" and preferred in PROVIDER_ORDER:
        order = [preferred] + [p for p in PROVIDER_ORDER if p != preferred]

    for name in order:
        env_key = KEY_MAP.get(name, "")
        api_key = os.environ.get(env_key, "").strip()
        if not api_key:
            print(f"  ‚è≠  {name}: no key ({env_key})")
            continue
        try:
            provider = create_provider(name)
            print(f"  ‚úÖ Using provider: {name}")
            return provider, name
        except Exception as exc:
            print(f"  ‚ö†Ô∏è  {name}: init failed ({exc})")
    return None, None


async def run_single_query(
    query: dict,
    provider,
    provider_name: str,
    output_dir: Path,
) -> dict:
    """Run a single canary query and return structured result."""
    from cdr.orchestration.graph import CDRRunner

    query_id = query["id"]
    question = query["question"]
    run_id = f"canary_{query_id.lower()}"
    query_dir = output_dir / run_id

    result = {
        "query_id": query_id,
        "question": question,
        "run_id": run_id,
        "status": "FAIL",
        "error": None,
        "warnings": [],
        "metrics": {},
    }

    try:
        query_dir.mkdir(parents=True, exist_ok=True)

        runner = CDRRunner(
            llm_provider=provider,
            output_dir=str(query_dir),
            dod_level=1,
        )

        t0 = time.time()
        final_state = await runner.run(
            research_question=question,
            max_results=10,
            formats=["markdown", "json"],
            run_id=run_id,
        )
        elapsed = time.time() - t0

        pipeline_status = final_state.status.value
        n_studies = len(final_state.study_cards)
        n_claims = len(final_state.claims)
        n_snippets = len(final_state.snippets)

        result["metrics"] = {
            "pipeline_status": pipeline_status,
            "latency_seconds": round(elapsed, 1),
            "study_count": n_studies,
            "claim_count": n_claims,
            "snippet_count": n_snippets,
        }

        # ‚îÄ‚îÄ Invariant checks (FAIL if violated) ‚îÄ‚îÄ

        # 1. Pipeline must not crash (we got here, so it didn't crash)

        # 2. Pipeline status must be completed or a valid scientific outcome
        valid_statuses = {"completed", "insufficient_evidence", "unpublishable"}
        if pipeline_status not in valid_statuses:
            result["error"] = f"Invalid pipeline status: {pipeline_status}"
            return result

        # 3. Export must generate files
        export_files = list(query_dir.glob("cdr_report_*"))
        if not export_files:
            # Try publisher export manually
            try:
                from cdr.publisher import Publisher
                from cdr.core.schemas import SynthesisResult

                publisher = Publisher(
                    output_dir=query_dir,
                    include_appendices=True,
                    include_verification=True,
                )
                synthesis_result = final_state.synthesis_result
                if synthesis_result is None:
                    synthesis_result = SynthesisResult(
                        claims=final_state.claims,
                        overall_narrative=final_state.answer or "No narrative.",
                    )
                pub_result = publisher.publish(
                    state=final_state,
                    synthesis_result=synthesis_result,
                    critique_result=None,
                    verification_results=None,
                    formats=["markdown", "json", "html"],
                )
                export_files = list(query_dir.glob("cdr_report_*"))
            except Exception as pub_err:
                result["error"] = f"Publisher export failed: {pub_err}"
                return result

        if not export_files:
            result["error"] = "No export files generated"
            return result

        result["metrics"]["export_files"] = len(export_files)

        # 4. Schema validation on the pipeline report
        report_json_files = list(query_dir.glob("cdr_report_*.json"))
        if report_json_files:
            try:
                import jsonschema

                schema_path = ROOT / "schemas" / "report.schema.json"
                if schema_path.exists():
                    schema = json.loads(schema_path.read_text())
                    report_data = json.loads(report_json_files[0].read_text())
                    jsonschema.validate(report_data, schema)
                    result["metrics"]["schema_valid"] = True
                else:
                    result["warnings"].append("Schema file not found, skipping validation")
            except jsonschema.ValidationError as ve:
                result["warnings"].append(f"Schema validation warning: {ve.message[:100]}")
                result["metrics"]["schema_valid"] = False
            except Exception as se:
                result["warnings"].append(f"Schema check error: {str(se)[:100]}")

        # 5. Claim count coherence
        if n_claims > 0:
            result["metrics"]["coherence_ok"] = True
        elif pipeline_status == "completed":
            result["warnings"].append("Pipeline completed but 0 claims generated")
            result["metrics"]["coherence_ok"] = False

        # ‚îÄ‚îÄ Soft warnings (don't fail) ‚îÄ‚îÄ
        if elapsed > 600:
            result["warnings"].append(f"High latency: {elapsed:.0f}s > 600s threshold")
        if n_studies == 0 and pipeline_status == "completed":
            result["warnings"].append("0 studies included despite completed status")

        # ‚îÄ‚îÄ Sanitize all output files ‚îÄ‚îÄ
        for f in query_dir.rglob("*"):
            if f.is_file() and f.suffix in {".json", ".md", ".html", ".txt"}:
                content = f.read_text(encoding="utf-8", errors="replace")
                clean = sanitize_text(content)
                if clean != content:
                    f.write_text(clean, encoding="utf-8")

        # ‚îÄ‚îÄ PASS ‚îÄ‚îÄ
        result["status"] = "PASS"
        return result

    except Exception as exc:
        result["error"] = sanitize_text(str(exc)[:300])
        return result


async def main():
    parser = argparse.ArgumentParser(description="CDR Online Canary")
    parser.add_argument("--output", default="canary_output", help="Output directory")
    parser.add_argument("--provider", default="auto", help="LLM provider (auto|openrouter|groq|cerebras|gemini)")
    args = parser.parse_args()

    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)

    print("=" * 60)
    print("üê§ CDR Online Canary ‚Äî Real E2E Validation")
    print("=" * 60)

    # ‚îÄ‚îÄ Select provider ‚îÄ‚îÄ
    print("\nüîç Selecting LLM provider...")
    provider, provider_name = pick_provider(args.provider)
    if provider is None:
        print("‚ùå No LLM provider available. Set at least one API key.")
        sys.exit(1)

    # ‚îÄ‚îÄ Run queries ‚îÄ‚îÄ
    query_results = []
    total_start = time.time()

    for i, query in enumerate(CANARY_QUERIES, 1):
        print(f"\n{'‚îÄ' * 60}")
        print(f"üìã Query {i}/{len(CANARY_QUERIES)}: {query['id']}")
        print(f"   {query['question'][:80]}...")
        print(f"{'‚îÄ' * 60}")

        result = await run_single_query(query, provider, provider_name, output_dir)
        query_results.append(result)

        status_icon = "‚úÖ" if result["status"] == "PASS" else "‚ùå"
        metrics = result.get("metrics", {})
        print(f"   {status_icon} {result['status']}"
              f"  latency={metrics.get('latency_seconds', '?')}s"
              f"  claims={metrics.get('claim_count', '?')}"
              f"  studies={metrics.get('study_count', '?')}")
        if result.get("error"):
            print(f"   ‚ùå Error: {result['error']}")
        for w in result.get("warnings", []):
            print(f"   ‚ö†Ô∏è  {w}")

    total_elapsed = time.time() - total_start

    # ‚îÄ‚îÄ Build summary ‚îÄ‚îÄ
    passed = sum(1 for r in query_results if r["status"] == "PASS")
    failed = sum(1 for r in query_results if r["status"] == "FAIL")
    all_warnings = []
    for r in query_results:
        all_warnings.extend(r.get("warnings", []))

    summary = {
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "provider": provider_name,
        "total_queries": len(CANARY_QUERIES),
        "passed": passed,
        "failed": failed,
        "total_latency_seconds": round(total_elapsed, 1),
        "warnings": all_warnings,
        "query_results": query_results,
    }

    summary_text = json.dumps(summary, indent=2, ensure_ascii=False, default=str)
    summary_text = sanitize_text(summary_text)

    summary_path = output_dir / "canary_summary.json"
    summary_path.write_text(summary_text, encoding="utf-8")

    # ‚îÄ‚îÄ Print final report ‚îÄ‚îÄ
    print(f"\n{'=' * 60}")
    print("üìä CDR ONLINE CANARY ‚Äî RESULTS")
    print(f"{'=' * 60}")
    print(f"  Provider:       {provider_name}")
    print(f"  Total queries:  {len(CANARY_QUERIES)}")
    print(f"  Passed:         {passed}")
    print(f"  Failed:         {failed}")
    print(f"  Total latency:  {total_elapsed:.1f}s")
    print(f"  Warnings:       {len(all_warnings)}")
    print()

    for r in query_results:
        icon = "‚úÖ" if r["status"] == "PASS" else "‚ùå"
        m = r.get("metrics", {})
        print(f"  {icon} {r['query_id']}: {r['status']}"
              f"  ({m.get('latency_seconds', '?')}s, "
              f"{m.get('claim_count', '?')} claims, "
              f"{m.get('study_count', '?')} studies)")

    print()
    if failed > 0:
        print(f"‚ùå CANARY FAILED: {failed}/{len(CANARY_QUERIES)} queries failed")
        sys.exit(1)
    else:
        print(f"‚úÖ CANARY PASSED: {passed}/{len(CANARY_QUERIES)} queries OK")

    print(f"\nüì¶ Artifacts: {output_dir}/")
    for p in sorted(output_dir.rglob("*")):
        if p.is_file():
            sz = p.stat().st_size
            rel = p.relative_to(output_dir)
            print(f"   {str(rel):<50} {sz:>8,} bytes")
    print(f"{'=' * 60}")


if __name__ == "__main__":
    asyncio.run(main())
